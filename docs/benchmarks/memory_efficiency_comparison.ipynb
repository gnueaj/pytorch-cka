{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOQFIYJTIB47"
   },
   "source": "# Memory Efficiency Comparison: pytorch-cka vs torch-cka\n\nThis notebook benchmarks GPU memory management between `pytorch-cka` (this library) and `torch-cka` using CIFAR-10 and ResNet-18 self-comparison.\n\n**Key metrics:**\n1. **Memory during computation** - How memory grows (or stays flat) as batches are processed\n2. **Memory after computation** - Memory retained after computation completes (not deallocated)\n\n**Why this matters:**\n- `pytorch-cka`: Clears features after each batch → constant memory usage during computation\n- `torch-cka`: Accumulates activations → memory grows with each batch and remains after computation"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SgMzkkiqIB49"
   },
   "outputs": [],
   "source": [
    "!pip install torch-cka pytorch-cka -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "R-sNQ58BIB4_"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from cka import CKA as PytorchCKA\n",
    "from torch_cka import CKA as TorchCKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhYUJ2HYIB4_",
    "outputId": "e3ddef73-f9b9-41ac-a591-884ad92f399d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Total GPU memory: 79.32 GB\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is required for GPU memory benchmarking. Please run on a GPU-enabled machine.\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TPZCJCdYIB5A"
   },
   "outputs": [],
   "source": [
    "def measure_memory_retention(func):\n",
    "    \"\"\"Measure memory retained after computation (not deallocated).\n",
    "\n",
    "    This captures the key difference: pytorch-cka deallocates memory\n",
    "    after computation, while torch-cka retains it.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    mem_before = torch.cuda.memory_allocated()\n",
    "\n",
    "    result = func()\n",
    "\n",
    "    # Force cleanup attempt\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    mem_after = torch.cuda.memory_allocated()\n",
    "    peak_mb = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "    retained_mb = (mem_after - mem_before) / (1024**2)\n",
    "\n",
    "    return result, peak_mb, retained_mb"
   ]
  },
  {
   "cell_type": "code",
   "source": "def measure_memory_at_batch_counts(cka_benchmark_func, model_fn, full_dataloader, batch_counts):\n    \"\"\"Measure retained memory after processing different numbers of batches.\n\n    This shows how memory accumulates (or not) as more data is processed.\n\n    Args:\n        cka_benchmark_func: Function that runs CKA (benchmark_pytorch_cka or benchmark_torch_cka)\n        model_fn: Function that creates a fresh model\n        full_dataloader: DataLoader with all data\n        batch_counts: List of batch counts to test [1, 2, 4, 8, ...]\n\n    Returns:\n        memory_by_batch_count: List of retained memory (MB) after each batch count\n    \"\"\"\n    memory_by_batch_count = []\n    batch_size = full_dataloader.batch_size\n\n    for num_batches in batch_counts:\n        # Create subset with exactly num_batches worth of data\n        subset_size = min(num_batches * batch_size, len(full_dataloader.dataset))\n        subset_indices = list(range(subset_size))\n        subset_data = Subset(full_dataloader.dataset, subset_indices)\n        subset_loader = DataLoader(subset_data, batch_size=batch_size, shuffle=False, num_workers=0)\n\n        # Create fresh model\n        model = model_fn().to(device)\n        model.eval()\n\n        # Measure memory after running CKA\n        gc.collect()\n        torch.cuda.empty_cache()\n        baseline = torch.cuda.memory_allocated()\n\n        cka_benchmark_func(model, subset_loader)\n\n        # Measure retained memory (before explicit cleanup)\n        retained_mb = (torch.cuda.memory_allocated() - baseline) / (1024**2)\n        memory_by_batch_count.append(retained_mb)\n\n        # Cleanup for next iteration\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    return memory_by_batch_count",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHsbfjx5IB5B",
    "outputId": "29a6a7a0-2651-44dc-e3bd-06595da77b68"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 170M/170M [00:13<00:00, 12.3MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset size: 1000 samples\n"
     ]
    }
   ],
   "source": [
    "def get_resnet18_cifar(num_classes=10):\n",
    "    \"\"\"ResNet-18 adapted for CIFAR-10 (smaller input size).\"\"\"\n",
    "    model = models.resnet18(weights=None)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# CIFAR-10 transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Download CIFAR-10\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Use a subset for faster benchmarking (1000 samples)\n",
    "subset_indices = list(range(1000))\n",
    "subset = Subset(dataset, subset_indices)\n",
    "\n",
    "print(f\"Dataset size: {len(subset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HF8Wq_qAIB5C"
   },
   "outputs": [],
   "source": "def benchmark_pytorch_cka(model, dataloader):\n    \"\"\"Run pytorch-cka self-comparison.\"\"\"\n    with PytorchCKA(model, model, device=device) as cka:\n        cka.compare(dataloader, progress=False)\n\ndef benchmark_torch_cka(model, dataloader):\n    \"\"\"Run torch-cka self-comparison.\"\"\"\n    cka = TorchCKA(\n        model, model,\n        model1_name=\"ResNet18\",\n        model2_name=\"ResNet18\",\n        device=device\n    )\n    with torch.no_grad():\n        cka.compare(dataloader, dataloader)"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUl02paHIB5C",
    "outputId": "81b20579-42af-4ba1-a4b7-2320b20a0e53"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running memory benchmarks...\n",
      "\n",
      "Batch    pytorch-cka              torch-cka               \n",
      "Size     Peak (MB)    Retained (MB) Peak (MB)    Retained (MB)\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch_cka/cka.py:51: UserWarning: Both model have identical names - ResNet18. It may cause confusion when interpreting the results. Consider giving unique names to the models :)\n",
      "  warn(f\"Both model have identical names - {self.model2_info['Name']}. \" \\\n",
      "| Comparing features |: 100%|██████████| 16/16 [01:07<00:00,  4.24s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "64       367.57       9.12         387.35       197.08      \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "| Comparing features |: 100%|██████████| 8/8 [00:35<00:00,  4.41s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "128      673.32       0.00         719.83       489.71      \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "| Comparing features |: 100%|██████████| 4/4 [00:24<00:00,  6.19s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "256      1306.60      0.00         1384.71      1090.96     \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "| Comparing features |: 100%|██████████| 2/2 [00:19<00:00,  9.56s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "512      2664.05      0.00         2715.71      2289.22     \n"
     ]
    }
   ],
   "source": [
    "# Batch sizes to test\n",
    "batch_sizes = [64, 128, 256, 512]\n",
    "\n",
    "# Results storage\n",
    "pytorch_cka_peak = []\n",
    "pytorch_cka_retained = []\n",
    "torch_cka_peak = []\n",
    "torch_cka_retained = []\n",
    "\n",
    "print(\"Running memory benchmarks...\\n\")\n",
    "print(f\"{'Batch':<8} {'pytorch-cka':<24} {'torch-cka':<24}\")\n",
    "print(f\"{'Size':<8} {'Peak (MB)':<12} {'Retained (MB)':<12} {'Peak (MB)':<12} {'Retained (MB)':<12}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Create fresh model for each test\n",
    "    model = get_resnet18_cifar().to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Benchmark pytorch-cka\n",
    "    _, peak1, retained1 = measure_memory_retention(lambda: benchmark_pytorch_cka(model, dataloader))\n",
    "    pytorch_cka_peak.append(peak1)\n",
    "    pytorch_cka_retained.append(retained1)\n",
    "\n",
    "    # Clear and recreate model\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = get_resnet18_cifar().to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Benchmark torch-cka\n",
    "    _, peak2, retained2 = measure_memory_retention(lambda: benchmark_torch_cka(model, dataloader))\n",
    "    torch_cka_peak.append(peak2)\n",
    "    torch_cka_retained.append(retained2)\n",
    "\n",
    "    print(f\"{batch_size:<8} {peak1:<12.2f} {retained1:<12.2f} {peak2:<12.2f} {retained2:<12.2f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Memory timeline: How memory grows (or stays flat) as more batches are processed\n# This directly compares memory allocation DURING computation\n\nprint(\"Running memory timeline benchmark...\")\nprint(\"(Measuring retained memory after processing 1, 2, 4, 8, 16 batches)\\n\")\n\ntimeline_batch_size = 64\ntimeline_dataloader = DataLoader(subset, batch_size=timeline_batch_size, shuffle=False, num_workers=0)\nbatch_counts = [1, 2, 4, 8, 16]\n\n# Measure memory timeline for pytorch-cka\nprint(\"Testing pytorch-cka...\")\npytorch_memory_timeline = measure_memory_at_batch_counts(\n    benchmark_pytorch_cka, get_resnet18_cifar, timeline_dataloader, batch_counts\n)\n\n# Measure memory timeline for torch-cka\nprint(\"Testing torch-cka...\")\ntorch_memory_timeline = measure_memory_at_batch_counts(\n    benchmark_torch_cka, get_resnet18_cifar, timeline_dataloader, batch_counts\n)\n\nprint(\"\\nMemory Timeline Results:\")\nprint(f\"{'Batches':<10} {'pytorch-cka (MB)':<20} {'torch-cka (MB)':<20}\")\nprint(\"-\" * 50)\nfor i, num_batches in enumerate(batch_counts):\n    print(f\"{num_batches:<10} {pytorch_memory_timeline[i]:<20.2f} {torch_memory_timeline[i]:<20.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "rP8c47TfIB5C",
    "outputId": "348d4420-1b9e-49d5-a372-f5904844f2dd"
   },
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nx = np.arange(len(batch_sizes))\nwidth = 0.35\n\n# Left: Memory timeline (during computation)\nax0 = axes[0]\nax0.plot(batch_counts, pytorch_memory_timeline, 'o-', label='pytorch-cka', color='#2ecc71', linewidth=2, markersize=8)\nax0.plot(batch_counts, torch_memory_timeline, 'o-', label='torch-cka', color='#e74c3c', linewidth=2, markersize=8)\nax0.set_xlabel('Number of Batches Processed')\nax0.set_ylabel('Memory Allocated (MB)')\nax0.set_title('Memory During Computation\\n(grows vs stays flat)')\nax0.legend()\nax0.grid(alpha=0.3)\nax0.set_xticks(batch_counts)\n\n# Middle: Peak memory comparison\nax1 = axes[1]\nax1.bar(x - width/2, pytorch_cka_peak, width, label='pytorch-cka', color='#2ecc71')\nax1.bar(x + width/2, torch_cka_peak, width, label='torch-cka', color='#e74c3c')\nax1.set_xlabel('Batch Size')\nax1.set_ylabel('Peak GPU Memory (MB)')\nax1.set_title('Peak Memory During Computation')\nax1.set_xticks(x)\nax1.set_xticklabels(batch_sizes)\nax1.legend()\nax1.grid(axis='y', alpha=0.3)\n\n# Right: Retained memory comparison (after computation)\nax2 = axes[2]\nax2.bar(x - width/2, pytorch_cka_retained, width, label='pytorch-cka', color='#2ecc71')\nax2.bar(x + width/2, torch_cka_retained, width, label='torch-cka', color='#e74c3c')\nax2.set_xlabel('Batch Size')\nax2.set_ylabel('Retained GPU Memory (MB)')\nax2.set_title('Memory After Computation\\n(not deallocated)')\nax2.set_xticks(x)\nax2.set_xticklabels(batch_sizes)\nax2.legend()\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-NYVq1MIB5D",
    "outputId": "98707952-2ba9-4754-d48f-212d62b7518f"
   },
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"SUMMARY: Memory Efficiency Results\")\nprint(\"=\" * 70)\n\navg_pytorch_retained = np.mean(pytorch_cka_retained)\navg_torch_retained = np.mean(torch_cka_retained)\navg_pytorch_peak = np.mean(pytorch_cka_peak)\navg_torch_peak = np.mean(torch_cka_peak)\n\nprint(\"\\n1. MEMORY DURING COMPUTATION (as batches are processed)\")\nprint(\"-\" * 70)\nprint(f\"   pytorch-cka: Memory stays flat (~{pytorch_memory_timeline[-1]:.1f} MB regardless of batch count)\")\nprint(f\"   torch-cka:   Memory grows with batches ({torch_memory_timeline[0]:.1f} MB -> {torch_memory_timeline[-1]:.1f} MB)\")\ngrowth_rate = (torch_memory_timeline[-1] - torch_memory_timeline[0]) / (batch_counts[-1] - batch_counts[0])\nprint(f\"   torch-cka growth rate: ~{growth_rate:.1f} MB per batch\")\n\nprint(\"\\n2. MEMORY AFTER COMPUTATION (retained, not deallocated)\")\nprint(\"-\" * 70)\nprint(f\"   pytorch-cka: {avg_pytorch_retained:.2f} MB (properly deallocates)\")\nprint(f\"   torch-cka:   {avg_torch_retained:.2f} MB (retains memory)\")\nif avg_pytorch_retained > 0:\n    print(f\"   Ratio: torch-cka retains {avg_torch_retained / max(avg_pytorch_retained, 0.01):.1f}x more memory\")\n\nprint(\"\\n3. PEAK MEMORY\")\nprint(\"-\" * 70)\nprint(f\"   pytorch-cka: {avg_pytorch_peak:.2f} MB\")\nprint(f\"   torch-cka:   {avg_torch_peak:.2f} MB\")\n\nprint(\"\\n4. DETAILED RESULTS BY BATCH SIZE\")\nprint(\"-\" * 70)\nprint(f\"{'Batch':<8} {'pytorch-cka':<24} {'torch-cka':<24}\")\nprint(f\"{'Size':<8} {'Peak':<12} {'Retained':<12} {'Peak':<12} {'Retained':<12}\")\nfor i, bs in enumerate(batch_sizes):\n    print(f\"{bs:<8} {pytorch_cka_peak[i]:<12.2f} {pytorch_cka_retained[i]:<12.2f} \"\n          f\"{torch_cka_peak[i]:<12.2f} {torch_cka_retained[i]:<12.2f}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CONCLUSION:\")\nprint(\"- DURING computation: pytorch-cka maintains constant memory,\")\nprint(\"  while torch-cka memory grows linearly with the number of batches.\")\nprint(\"- AFTER computation: pytorch-cka deallocates GPU memory,\")\nprint(\"  while torch-cka retains activations in memory.\")\nprint(\"=\" * 70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "H100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}